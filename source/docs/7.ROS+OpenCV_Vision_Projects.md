# 7. ROS+OpenCV Vision Projects

<p id="anchor_7_1"></p>

## 7.1 Color Threshold Adjustment

Objects may appear differently in color under various light sources, which can affect games involving color recognition. This lesson will guide you through using the `LabConfig` tool to adjust the color threshold and mitigate this issue effectively.

### 7.1.1 Enable LabConfig

:::{Note}
The input command should be case-sensitive, and keywords can be complemented using `Tab` key.
:::

(1) Start ROSPug robot dog, and access the robot system desktop using [NoMachine](Appendix.md).

(2) Double-click <img src="../_static/media/chapter_7/section_1/image2.png" /> to start the `LabConfig` tool. If the software is enabled successfully, you can skip step (3) to (6).

(3) If `LabConfig` cannot be opened, double-click <img src="../_static/media/chapter_7/section_1/image3.png" /> to open the command-line terminal.

(4) Execute the command below:

```bash
roslaunch lab_config lab_config_manager.launch
```

(5) Run the following command, and hit `Enter` key to enable the camera service

```bash
roslaunch pug_peripherals camera.launch
```

(6) Double-click <img src="../_static/media/chapter_7/section_1/image2.png" /> to open the `LabConfig` tool.

### 7.1.2 Introduction to LabConfig Interface

The `LabConfig` interface is divided into two parts, including display area and parameter adjustment area.

<img src="../_static/media/chapter_7/section_1/image6.png" class="common_img" />

(1) Display area: The image on the left side is the camera-processed image, while the image on the right side is the original.

:::{Note}
If the live camera feed cannot be displayed properly, indicating a failure in camera connection, please ensure that the connection cable is securely plugged in.
:::

(2) Parameter adjustment area: adjust the color threshold parameters. The functions of the icons are listed below:

| **Icon**                                                              | **Function**                                                                                                            |
| :--------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------: |
| <img src="../_static/media/chapter_7/section_1/image7.png" /> | Drag the sliders L, A and B to adjust the L, A and B components of the live camera feed.                                |
| <img src="../_static/media/chapter_7/section_1/image8.png" /> | Select the color to be adjusted.                                                                                        |
| <img src="../_static/media/chapter_7/section_1/image9.png" /> | Apply the color threshold currently adjusted.                                                                           |
| <img src="../_static/media/chapter_7/section_1/image10.png" /> | Save the adjustment result.                                                                                             |
| <img src="../_static/media/chapter_7/section_1/image11.png" /> | Add new colors to be recognized.                                                                                        |

<p id="anchor_7_1_3"></p>

### 7.1.3 Adjust Color Threshold

(1) Start `LabConfig`. Select the color to be adjusted in the drop-down menu. Take adjusting red as example.

<img src="../_static/media/chapter_7/section_1/image12.png" class="common_img" />

(2) Set the **'min'** value of L, A and B components as **'0'**, and the **'max'** values to **'255'**.

<img src="../_static/media/chapter_7/section_1/image13.png" class="common_img" />

(3) Position the colored object within the camera's field of view, consult the LAB color space distribution map, and fine-tune the L, A, and B components to align with the desired color recognition range.

<img src="../_static/media/chapter_7/section_1/image14.png" class="common_img" />

If the red hue is closer to **'+a'**, adjust by increasing the A component. This involves maintaining the **'max'** value of the A component unchanged while increasing its **'min'** value until the color object area on the left side of the screen turns white, while the remaining area turns black.

<img src="../_static/media/chapter_7/section_1/image15.png" class="common_img" />

(4) Fine-tune the remaining L and B components based on the environmental conditions. If the red hue appears lighter, increase the **'min'** value of the L component. Conversely, if the red hue appears darker, decrease the **'max'** value of the L component. If the red hue appears warmer, increase the **'min'** value of the B component; if it appears cooler, reduce the **'max'** value of the B component.

The table below displays the parameter information for LAB threshold adjustment.

| **Color component** | **Range** | **Color zone**         |
| :-------------------: | :---------: | :--------------------: |
| L                   | 0~255     | Black-White（-L ~ +L） |
| A                   | 0~255     | Green-Red（-a ~ +a）   |
| B                   | 0~255     | Blue-Yellow（-b ~ +b） |

(5) Click-on **'Apply'** button to apply the color threshold parameters currently adjusted.

<img src="../_static/media/chapter_7/section_1/image16.png" class="common_img" />

(6) Click the **"Save"** button to preserve the color threshold adjustment parameters.

<img src="../_static/media/chapter_7/section_1/image17.png" class="common_img" />

### 7.1.4 Add New Recognition Color

In addition to the three pre-defined recognition colors, users have the option to add other identifiable colors. Here, purple will be used as an example. The specific steps are as follows:

(1) Open `LabConfig`, then click-on **'Add'** button.

<img src="../_static/media/chapter_7/section_1/image18.png" class="common_img" />

(2) Name the color, for example **'purple'**.

<img src="../_static/media/chapter_7/section_1/image19.png" class="common_img" />

(3) Position the colored object within the camera's field of view, then adjust the L, A, and B component sliders until the color object area on the left side of the screen turns white, while the remaining areas turn black.

<img src="../_static/media/chapter_7/section_1/image20.png" class="common_img" />

:::{Note}
To access the detailed instructions, please refer to section [7.1.3 Adjust Color Threshold](#anchor_7_1_3).
:::

(4) Click the **'Apply'** button to apply the currently adjusted color threshold parameters.

<img src="../_static/media/chapter_7/section_1/image21.png" class="common_img" />

(5) Click-on **'Save'** button to save the the currently adjusted color threshold parameters.

<img src="../_static/media/chapter_7/section_1/image22.png" class="common_img" />

## 7.2 Color Recognition

:::{Note}
*   If color recognition is inaccurate, please refer to [7.1 Color Threshold Adjustment](#anchor_7_1) for further adjustments.
*   Ensure there are no other objects in the camera's field of view that contain the recognized color, as this may affect gameplay effectiveness.
:::

### 7.2.1 Program Logic

First, the color must be identified. To achieve this, we utilize the Lab color space for processing, converting the image color space from RGB to Lab. Subsequently, operations such as binarization, erosion, and dilation are applied to the image to extract the maximum contour containing the target color. Finally, the color recognition results are provided back through both the display screen and the terminal interface.

### 7.2.2 Operation Steps

:::{Note}
The input command should be case sensitive.
:::

(1) Start ROSPug robot dog, and access the robot system desktop using [NoMachine](Appendix.md).

(2) Click-on <img src="../_static/media/chapter_7/section_2/image3.png" /> to open the command-line terminal.

(3) Execute the following command, and hit `Enter` key to initiate the color recognition program.

```bash
rosrun pug_example color_detect_demo.py
```

(4) If you need to terminate this program, use short-cut **'Ctrl+C'**. If the program cannot be stopped, please retry.

### 7.2.3 Program Outcome

When red, green, or blue is recognized, the color name will be printed in the lower left corner of the screen and also displayed in the terminal. Additionally, the target item will be highlighted with a circle of the corresponding color on the screen.

Take recognizing red sphere as example.

<img src="../_static/media/chapter_7/section_2/image5.png" class="common_img" />

### 7.2.4 Program Parameters Explanation

The source code of this program is saved in: [/home/pug/src/pug_example/scripts/detect_and_track/color_detect_demo.py](../_static/source_code/detect_and_track.zip)

* **Image Processing**

(1) Gaussian Filtering

Before converting the image color space from RGB to Lab, it undergoes denoising. The `GaussianBlur()` function from the `cv2` library is employed for this purpose, performing Gaussian filtering on the image.

{lineno-start=53}
```python
    frame_gb = cv2.GaussianBlur(frame_resize, (3, 3), 3) 
```
The parameters in the function brackets are as follows:

The first parameter, `frame_resize`, represents the input image.

The second parameter, `(3, 3)`, denotes the Gaussian kernel size.

The third parameter, `3`, indicates the variance allowed around the mean value in Gaussian filtering. A larger value allows greater variance around the mean, while a smaller value allows less variance around the mean.

(2) Thresholding Processing

Utilize `inRange()` function from `cv2` library to perform thresholding on the image.

{lineno-start=61}
```python
        if i in ['red', 'green', 'blue', 'purple']:
            frame_mask = cv2.inRange(frame_lab,
                                            (color_range_list[i]['min'][0],
                                            color_range_list[i]['min'][1],
                                            color_range_list[i]['min'][2]),
                                            (color_range_list[i]['max'][0],
                                            color_range_list[i]['max'][1],
                                            color_range_list[i]['max'][2])) 
```

The first parameter in the function brackets represents the input image. The second and third parameters denote the lower and upper limits of the threshold, respectively. When the RGB color value of a pixel falls within the specified range, the pixel is assigned a value of 1; otherwise, it is assigned a value of 0.

(3) Erosion and Dilation

To reduce interference and achieve smoother images, erosion and dilation operations are applied.

{lineno-start=69}
```python
            eroded = cv2.erode(frame_mask, cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3)))  
            dilated = cv2.dilate(eroded, cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3)))
```
The `erode()` function performs erosion operations on images. In the code example `eroded = cv2.erode(frame_mask, cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3)))`, the parameters in the brackets have the following meanings:

The first parameter, `frame_mask`, represents the input image.

The second parameter, `cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))`, defines the structural element or kernel that determines the operation's nature. The first parameter within parentheses specifies the kernel shape, and the second parameter specifies the kernel size.

Similarly, the `dilate()` function is used to dilate images. The parameters in its parentheses have the same meanings as those in the `erode()` function.

(4) Obtain the Contour with The Maximum Area

Upon completing the aforementioned image processing, the next step involves obtaining the contour of the recognized target, which utilizes the `findContours()` function in the `cv2` library. The parameters within the function brackets are as follows:

{lineno-start=71}
```python
            contours = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[-2]
```
① The first parameter represents the input image.

② The second parameter denotes the contour retrieval mode.

③ The third parameter specifies the contour approximation method.

Identify the contour with the largest area among the contours obtained. To minimize interference, a minimum threshold value must be set. Only when the area exceeds this threshold is the target contour considered valid.

{lineno-start=37}
```python
            if contour_area_temp > 50:  
                area_max_contour = c
```
(5) Feedback Information

After obtaining the contour with the maximum area, the recognition target is highlighted with a circle using the `circle()` function from the `cv2` library. The circle's color corresponds to the recognition color.

{lineno-start=83}
```python
        cv2.circle(img, (centerX, centerY), radius, range_rgb[color_area_max], 2)
```
Display the detected color on the returned image, using the `putText()` function from the `cv2` library.

{lineno-start=}
```python
    cv2.putText(img, "Color: " + detect_color, (10, img.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.65, draw_color, 2)
```
<img src="../_static/media/chapter_7/section_2/image13.png" class="common_img" />

The parameters within the function brackets have the following meanings:

① The first parameter, `img`, represents the input image.

② The second parameter, `("Color: " + detect_color)`, specifies the content to be displayed.

③ The third parameter, `(10, img.shape[0] - 10)`, determines the display position.

④ The fourth parameter, `cv2.FONT_HERSHEY_SIMPLEX`, defines the font type.

⑤ The fifth parameter, `0.65`, indicates the font size.

⑥ The sixth parameter, `draw_color`, represents the font color.

⑦ The seventh parameter, `2`, signifies the font thickness.

## 7.3 Color Localization & Tracking

:::{Note}
*   Please ensure that objects with the same or similar color as the one being tracked are not present within the camera's field of view, as this may affect game.
*   During tracking, ensure that the color block does not move too quickly.
*   If color recognition accuracy is unsatisfactory, refer to [7.1 Color Threshold Adjustment](#anchor_7_1) for guidance on adjustments.

The object is tracked using images captured by a high-definition camera. When the target color block is positioned within the camera's field of view, the leg servos will adjust their position to mimic the movement of the target color block.
:::

### 7.3.1 Program Logic

The camera captures an RGB image, which is then resized and processed with Gaussian blur. The image's color space is converted from RGB to LAB (for more details on the LAB color space, refer to OpenCV Computer Vision Course->Image Processing---Color Space Conversion.

Next, the color thresholds set in [7.1 Color Threshold Adjustment](#anchor_7_1) are used to match the image. Once the colors are detected, a morphological opening operation is applied, and the detected colors are highlighted with circles.

Opening Operation: Erosion followed by dilation. This removes small objects, smooths shape boundaries, and preserves the object's area. It also eliminates small noise particles and separates connected objects.

Erosion: Reduces the boundary points of objects, causing the boundary to shrink inward, and removes objects smaller than the structuring element.

Dilation: Expands the boundary of objects, merging background points into the object and causing the boundary to expand outward.

Finally, the center of the color block is calculated based on the circle's position. The positional deviation between the color block's center and the image center is then determined. A PID algorithm is used to adjust the robot in real-time, allowing the gimbal servo to follow the movement of the color block.

### 7.3.2 Operation Steps

:::{Note}
The input command should be case sensitive, and keywords can be complemented using `Tab` key.
:::

(1) Start ROSPug robot dog, and access the robot system desktop using [NoMachine](Appendix.md).

(2) Click-on <img src="../_static/media/chapter_7/section_3/image2.png" /> to open the command-line terminal.

(3) Execute the following command, and hit `Enter` key to initiate the color tracking program.

```bash
rosrun pug_example color_tracking_demo.py
```

(4) If you need to terminate this program, use short-cut **'Ctrl+C'**. If the program cannot be stopped, please retry.

### 7.3.3 Program Outcome

After activating the mode, position the purple ball in front of the HD camera. The returned image will show the detected color of the target, and the robot dog will track and follow the target's vertical movements.

<img src="../_static/media/chapter_7/section_3/image4.png" class="common_img" />

### 7.3.4 Code Analysis

The source code of this program is saved in :[/home/pug/src/pug_example/scripts/detect_and_track/color_tracking_demo.py](../_static/source_code/detect_and_track.zip)

* **Import Function Library**

If we wish to invoke a function from a function library, we use the syntax 'function library name + function name (parameter, parameter...).' For example:

{lineno-start=}
```python
paste source code here.
```

<img src="../_static/media/chapter_7/section_3/image6.png" class="common_img" />

To utilize the `sleep` function from the `rospy` library, we call `rospy.sleep()` where the `sleep()` function introduces a delay. Python includes several built-in libraries that can be imported and directly called, such as `cv2`, `math`, and so on.

* **Main Function Analysis**

(1) Create Command-Line Interface

To create the necessary command-line interface, we utilize the `argparse` module. The process involves creating a parser, adding parameters, and parsing these parameters.

{lineno-start=}
```python
paste source code here.
```
<img src="../_static/media/chapter_7/section_3/image7.png" class="common_img" />

In the code snippet `parser.add_argument('target_color', metavar="COLOR NAME", nargs='?', type=str, help="Color Name", default='red')`, the parameters have the following meanings:

(1) The first parameter, `target_color`, represents a name or a list of option strings.

(2) The second parameter, `metavar='COLOR NAME'`, serves as an example of the parameter value used in the help message.

(3) The third parameter, `nargs='?'`, specifies the number of command-line parameters that should be consumed.

(4) The fourth parameter, `type=str`, indicates the parameter type.

(5) The fifth parameter, `help="color name"`, provides a description of the option.

(6) The sixth parameter, `default="red"`, sets the default value for the parameter, meaning the target tracking color defaults to red.

* **Specify Target Color**

When running the program file, you can specify the target color by adding color parameters, such as green, blue, etc., at the end of the launch file.

{lineno-start=}
```python
paste source code here.
```

<img src="../_static/media/chapter_7/section_3/image8.png" class="common_img" />

* **Invoke the Sub-function for Color Tracking**

Utilize the `ColorTrackingNode` class to track one of the colors, namely red, green, or blue. For detailed analysis of the `ColorTrackingNode` class, please refer to [Sub-function Analysis](#anchor_7_3_4_5) section.

{lineno-start=}
```python
paste source code here.
```

<img src="../_static/media/chapter_7/section_3/image9.png" class="common_img" />

<p id="anchor_7_3_4_5"></p>

* **Sub-function Analysis**

(1) Define Initialization Function

(1) Initialization

Initialize the target color, color threshold, PID parameters, and pan-tilt servo position.

{lineno-start=}
```python
paste source code here.
```

<img src="../_static/media/chapter_7/section_3/image10.png" class="common_img" />

(2) Subscribe to and Publish Node

Subscribe to the topic messages published by the camera node to obtain real-time images from the camera. Then, call the `image_callback` function to identify, locate, and track colors in the image. For analysis of the `image_callback` function, please refer to [Define Color Positioning and Tracking](#anchor_7_3_4_5_2) section.

{lineno-start=}
```python
paste source code here.
```

<img src="../_static/media/chapter_7/section_3/image11.png" class="common_img" />

<p id="anchor_7_3_4_5_2"></p>

(2) Define Color Positioning and Tracking

Firstly, identify the color and mark it with a circle once identified. Next, compare the position of the color block in the image with the center point of the returned image. Use the PID algorithm to adjust the position of the pan-tilt camera, ensuring that the object remains centered in the returned image. This ensures that the pan-tilt servo moves in tandem with the movement of the target object, resulting in a smooth tracking effect.

(1) Convert the ROS-format Image to OpenCV Format

{lineno-start=}
```python
paste source code here.
```

<img src="../_static/media/chapter_7/section_3/image12.png" class="common_img" />

(2) Scaling

Utilize the `resize()` function from the `cv2` library to scale down the image and reduce computational workload.

{lineno-start=}
```python
paste source code here.
```

<img src="../_static/media/chapter_7/section_3/image12.png" class="common_img" />

In the function brackets, the first parameter, `rgb_image`, represents the input image. The second parameter, `int(ros_image.width/2), int(ros_image.height/2)`, specifies the width and height of the scaled image.

(3) Gaussian Filtering

Apply Gaussian filtering to the image using the `GaussianBlur()` function from the `cv2` library to reduce noise.

{lineno-start=}
```python
paste source code here.
```

<img src="../_static/media/chapter_7/section_3/image13.png" class="common_img" />

① The first parameter, `rgb_image`, represents the input image.

② The second parameter, `(3, 3)`, specifies the size of the Gaussian convolution kernel. Both dimensions must be positive and odd numbers.

③ The third parameter, `3`, denotes the standard deviation of the Gaussian kernel in the horizontal direction.

(4) Convert the Image to the LAB Color Space

{lineno-start=}
```python
paste source code here.
```

<img src="../_static/media/chapter_7/section_3/image14.png" class="common_img" />

(5) Thresholding

Utilize `inRange()` function from `cv2` library to perform thresholding on the image.

{lineno-start=}
```python
paste source code here.
```

<img src="../_static/media/chapter_7/section_3/image15.png" class="common_img" />

The first parameter in the function brackets represents the input image. The second and third parameters denote the lower and upper limits of the threshold, respectively. When the RGB color value of a pixel falls within the specified range, the pixel is assigned a value of 1; otherwise, it is assigned a value of 0.

(6) Erosion and Dilation

To reduce interference and achieve smoother images, erosion and dilation operations are applied.

{lineno-start=}
```python
paste source code here.
```

<img src="../_static/media/chapter_7/section_3/image15.png" class="common_img" />

The `erode()` function performs erosion operations on images. In the code example `eroded = cv2.erode(mask, cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3)))`, the parameters in the brackets have the following meanings:

① The first parameter, `mask`, represents the input image.

② The second parameter, `cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))`, defines the structural element or kernel that determines the operation's nature. The first parameter within parentheses specifies the kernel shape, and the second parameter specifies the kernel size.

Similarly, the `dilate()` function is used to dilate images. The parameters in its parentheses have the same meanings as those in the `erode()` function.

(7) Obtain the Contour with The Maximum Area

Utilize the `findContours()` function from the `cv2` library to identify the largest contour of the target recognition color within the image.

{lineno-start=}
```python
paste source code here.
```

<img src="../_static/media/chapter_7/section_3/image15.png" class="common_img" />

① The first parameter represents the input image.

② The second parameter denotes the contour retrieval mode.

③ The third parameter specifies the contour approximation method.

(8) Circle the Target

{lineno-start=}
```python
paste source code here.
```

<img src="../_static/media/chapter_7/section_3/image16.png" class="common_img" />

(9) Update PID Data

Update X-axis and Y-axis coordinate through PID algorithm.

{lineno-start=}
```python
paste source code here.
```

<img src="../_static/media/chapter_7/section_3/image17.png" class="common_img" />

Firstly, divide the height or width of the screen by 2 to obtain the center position coordinates of the screen. Then, compare it with the center point of the target color to calculate the deviation. Update the PID data information accordingly. Finally, use the output results to update the X-axis and Y-axis coordinates.

(10) Set the Rotation of Servo

{lineno-start=}
```python
paste source code here.
```

<img src="../_static/media/chapter_7/section_3/image17.png" class="common_img" />

Taking the code `serial_servo.set_position(19, int(self.yaw), 0)` as an example, the parameters in the function brackets have the following meanings:

① The first parameter `19` represents the ID number of the servo, where ID `19` corresponds to the pan-tilt servo.

② The second parameter `int(self.yaw)` indicates the rotation position of the servo. This value is determined by the PID algorithm and falls within the range of 0 to 1000. It represents the pulse width, with the conversion formula between pulse width and angle being pulse = 0.24×angle (Note: This formula is for reference only).

③ The third parameter `0` denotes the rotation time of the servo, measured in milliseconds.

## 7.4 KCF Target Tracking

:::{Note}
Make sure to keep the object within the camera's view, and avoid moving too quickly. If the object goes out of the camera's range, tracking will stop. To resume tracking, simply reselect the object.
:::

In this lesson, we will employ the KCF algorithm to use the mouse to frame the target. The ROSPug robot dog will then identify and frame the target, displaying its coordinates on the terminal.

### 7.4.1 Introduction to KCF

KCF (Kernel Correlation Filter) is an algorithm proposed by Joao F. Henriques, Rui Caseiro, Pedro Martins, and Jorge Batista in 2014. It is a discriminative tracking method that utilizes the cyclic matrix of the area surrounding the target to gather positive and negative samples. These samples are then used to train a target detector. This detector is employed to determine if the predicted position in the next frame matches the target. Based on this detection, the training set and consequently the object detector are updated.

### 7.4.2 Program Logic

First, build a tracker using OpenCV's KCF object tracking algorithm and subscribe to topic messages to receive the real-time camera feed.

Next, use a mouse callback function to select the tracking target in the returned image. The tracker will then provide the real-time position of the target, which is updated using a PID algorithm.

Finally, control the ROSPug to track the target vertically based on its position.

### 7.4.3 Operation Steps

:::{Note}
The input command should be case sensitive, and keywords can be complemented using `Tab` key.
:::

(1) Start the robot dog, and access the robot system desktop using [NoMachine](Appendix.md).

(2) Double-click <img src="../_static/media/chapter_7/section_4/image2.png" /> to open the command-line terminal.

(3) Execute the following command and hit `Enter` key to disable the auto-start service.

```bash
rosrun pug_example kcf_tracking.py
```

(4) If you need to terminate the program, use short-cut **'Ctrl+C'** to stop the program.

### 7.4.4 Program Outcome

After starting the game, position the target tracking object within the camera's field of view. Press **'s'** and use the left mouse button to select it on the screen. While selecting, the chosen area will be highlighted with a blue box. Release the left mouse button and press **'Space'** or **'Enter'** on the keyboard to confirm the selection; the box will turn green, indicating tracking can begin. If you need to reselect the tracking target, right-click the mouse on the screen to clear it, then repeat the selection process as described above.

### 7.4.5 Program Analysis

The source code is saved in : [/home/pug/src/pug_example/scripts/detect_and_track/kcf_tracking.py](../_static/source_code/detect_and_track.zip)

* **Initialize Configuration**

First, the script imports required libraries such as `cv2`, `rospy`, `queue`, `numpy`, etc. Then, it defines a class called `KCFTrackingNode` that encapsulates the node's logic. In the class's initialization method, it performs the following steps:

(1) It calls `rospy.init_node('kcf_node')` to initialize a ROS node named `kcf_node`.

(2) It instantiates a KCF tracker object `self.tracker` and sets it to `None`, indicating that tracking has not started yet.

(3) It sets a flag `self.enable_select` to `False`, indicating that the tracking target has not been selected.

(4) It instantiates a frame rate statistic object `self.fps`, which is used to display the frame rate of the picture.

(5) It instantiates two PID controller objects, `self.pid_pitch` and `self.pid_yaw`, which are used to control the servos for pitch and yaw angles.

(6) It initializes two variables, `self.pitch` and `self.yaw`, which represent the servo values for pitch and yaw angles, respectively. These are set to 500, corresponding to 120 degrees.

(7) It calls `serial_servo.set_position(19, 500, 1000)` to set the position of servo `19` to `500` and provide `1000` milliseconds of movement time.

(8) It subscribes to the camera image topic `self.camera_rgb_prefix + '/image_raw'` and sets the callback function to `self.image_callback`. Here, a parameter `/camera_rgb_prefix` is used to obtain the prefix of the camera topic, which defaults to `camera/rgb`.

{lineno-start=16}
```python
    def __init__(self, name):
        rospy.init_node(name)
        self.name = name
        self.y_dis = 0
        self.y_pid = pid.PID(P=0.0005, I=0, D=0)

        self.image_queue = queue.Queue(maxsize=2)
        self.fps = FPS()
        signal.signal(signal.SIGINT, self.shutdown)
        self.running = True
        self.mouse_click = False
        self.selection = None  
        self.track_window = None  
        self.drag_start = None   
        self.start_circle = True
        self.start_click = False
        self.params = cv2.TrackerNano_Params()
        model_path = os.path.split(os.path.realpath(__file__))[0]
        self.params.backbone = os.path.join(model_path, 'nanotrack_backbone_sim.onnx')
        self.params.neckhead = os.path.join(model_path, 'nanotrack_head_sim.onnx')
        self.tracker = cv2.TrackerNano_create(self.params)

        cv2.namedWindow(name, 1)
        cv2.setMouseCallback(name, self.onmouse)
         
        self.image_sub = rospy.Subscriber("/csi_camera/image_rect_color", Image, self.image_callback, queue_size=1)
        self.pose_pub = rospy.Publisher('/pug_control/pose', Pose, queue_size=1)
        self.run_action_group_srv = rospy.ServiceProxy('/pug_control/run_action_group', SetActionName)
        time.sleep(0.2)
        self.run_action_group_srv('stand')
        self.run()
```

* **Select and Track Target**

(1) Convert the received ROS image message to an `rgb_image` in OpenCV format and scale and copy it, using a factor to record the scaling ratio.

(2) Obtain the height (`h`) and width (`w`) of the image.

(3) Implement a `try-except` structure to catch potential exceptions.

(4) If `self.tracker` is `None`, indicating tracking has not yet begun, determine if the selection mode `self.enable_select` is enabled. If enabled, utilize `cv2.selectROI("image", cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR), False)` to select a region of interest (ROI) on `result_image` and convert it to coordinates in the `rgb_image` coordinate system. If the ROI is not empty, instantiate a KCF tracker object and initialize the tracking target to the ROI using its `init` method.

(5) If `self.tracker` is not `None`, implying tracking has commenced, call its `update` method to update the tracking status and location box. If the status is `True`, indicating successful tracking, execute the following steps:

① Log the value of the box. Convert the box to a value in the coordinate system of `result_image` based on the factor, and draw rectangular boxes (`p1` and `p2`) on the `result_image` to indicate the tracking target's position.

② Compute the coordinates `center_x` and `center_y` of the tracking target's center point in the image, and print them.

③ If the difference between `center_x` and 0.5 exceeds 0.05, indicating the tracking target is not centered in the image, utilize the PID controller `self.pid_yaw` to adjust the value of the yaw angle servo `self.yaw`. Here, the set value (`SetPoint`) of `self.pid_yaw` is set to 0.5, indicating the expected center position of the tracking target. Update the PID output according to the actual value of `center_x` and add it to `self.yaw`. Otherwise, clear the PID output.

④ Restrict the value of `self.yaw` within the range of 0 to 100 to prevent the servo from exceeding physical limits. Print the values of `self.pitch` and `self.yaw` in the log.

⑤ Invoke `serial_servo.set_position(19, int(self.yaw), 0)` to set the position of servo `19` to `self.yaw` with 0 milliseconds of movement time, i.e., immediate movement.

(6) If the status is set to `False`, it indicates the tracking ends in failure. The program will then clear PID output.

{lineno-start=52}
```python
    def image_callback(self, ros_image: Image):
        rgb_image = np.ndarray(shape=(ros_image.height, ros_image.width, 3), dtype=np.uint8,
                               buffer=ros_image.data)  
        if self.image_queue.full():
             
            self.image_queue.get()
       
        self.image_queue.put(rgb_image)

    
    def onmouse(self, event, x, y, flags, param):
        if event == cv2.EVENT_LBUTTONDOWN:   
            self.mouse_click = True
            self.drag_start = (x, y)   
            self.track_window = None
        if self.drag_start:  
            xmin = min(x, self.drag_start[0])
            ymin = min(y, self.drag_start[1])
            xmax = max(x, self.drag_start[0])
            ymax = max(y, self.drag_start[1])
            self.selection = (xmin, ymin, xmax, ymax)
        if event == cv2.EVENT_LBUTTONUP:   
            self.mouse_click = False
            self.drag_start = None
            self.track_window = self.selection
            self.selection = None
        if event == cv2.EVENT_RBUTTONDOWN:
            self.mouse_click = False
            self.selection = None   
            self.track_window = None   
            self.drag_start = None  
            self.start_circle = True
            self.start_click = False
            self.run_action_group_srv('stand')
            self.tracker = cv2.TrackerNano_create(self.params)
```

* **Status Monitoring and Exception Handling**

(1) In the `except` block of the `try-except` structure, print the exception information.

(2) Call `self.fps.update()` to update the frame rate statistics.

(3) Use `self.fps.show_fps(result_image)` to display the frame rate information on `result_image`.

(4) Convert `result_image` to the BGR format and display it in a window named **"image"**.

(5) Utilize `cv2.waitKey(1)` to capture user key input and assign it to the variable `key`. If the `key` is `ord('s')`, indicating the user pressed the **'s'** key, set `self.tracker` to `None` and `self.enable_select` to `True`, initiating the selection of the tracking target.

(6) Outside the class, employ the `if __name__ == '__main__'` structure to check if the main program is running. If so, execute the following actions:

① Employ a `try-except` structure to handle potential exceptions.

② Create a `KCFTrackingNode` object and assign it the name `kcf_tracking`.

③ Display four messages prompting the user to press the **'s'** key in the window to initiate the selection of the tracking target.

④ Invoke `rospy.spin()` to maintain the node's execution until termination.

⑤ In the `except` block of the `try-except` structure, print the exception information.

{lineno-start=138}

```python
            except BaseException as e:
                print(e)
                result_image = image
            cv2.imshow(self.name, cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))
            cv2.waitKey(1)
        self.run_action_group_srv('stand')
        rospy.signal_shutdown('shutdown')

if __name__ == '__main__':
    KCFTrackingNode('kcf_tracking')
```

## 7.5 Autonomous Line Following

### 7.5.1 Program Logic

Line following is a common challenge in robot competitions. While traditional methods often rely on two or four-channel sensors, the ROSPug robot achieves autonomous line following using its visual module to detect line colors and process images.

The process begins with identifying the line color using the Lab color space. The image is converted from RGB to Lab and then processed with binarization, erosion, and dilation to isolate the contours of the target color, which are highlighted with rectangles.

Once the color is recognized, the robot uses image feedback to calculate the line's position. It then adjusts the ROSPug's movement to follow the line and displays the line coordinates on the terminal, enabling effective autonomous line-following.

### 7.5.2 Operation Steps

:::{Note}
The input command should be case and space sensitive.
:::

(1) Start the robot dog, and access the robot system desktop using [NoMachine](Appendix.md).

(2) <img src="../_static/media/chapter_7/section_5/image3.png" /> Double-click to open the command-line terminal.

(3) Execute the command to terminate the app auto-start service.

```bash
rosrun pug_example visual_patrol_demo.py
```

(4) If you need to terminate this game, press **'Ctrl+C'** on the terminal. If the program cannot be stopped, please retry.

### 7.5.3 Program Outcome

:::{Note}
The default detection color is black.
:::
Use black electrical tape to set a path, and put ROSPug on the black line. After initiating the game, the robot dog will move along the black line.

### 7.5.4 Function Extension

* **Change Followed Color**

By default, the line-following color is set to black. To change the line-following color to a different one, such as red, please follow these steps:

(1) Execute the command and hit `Enter` key to edit the autonomous line following program.

```bash
vim visual_patrol_demo.py
```

(2) Locate the code as indicated in red frame below.

<img src="../_static/media/chapter_7/section_5/image7.png" class="common_img" />

(3) Press **'i'** key to enter the editing mode, then change **'black'** to **'red'**. After modification, press **'Esc'** key, input **':wq'** and press `Enter` key to save and exit the editing.

<img src="../_static/media/chapter_7/section_5/image8.png" class="common_img" />

(4) Run the following command to initiate the game, and check the game effect.

```bash
rosrun pug_example visual_patrol_demo.py
```

### 7.5.5 Program Parameter Analysis

The source code of this program is saved in: [/home/hiwonder/pug/src/pug_example/scripts/almighty_functions/visual_patrol_demo.py](../_static/source_code/detect_and_track.zip)

* **Subscribe to a Publishing Topic**

{lineno-start=38}
```python
        self.image_sub = rospy.Subscriber("/csi_camera/image_rect_color", Image, self.image_callback, queue_size=1)
        self.pose_pub = rospy.Publisher('/pug_control/pose', Pose, queue_size=1)
        self.gait_pub = rospy.Publisher('/pug_control/gait', Gait, queue_size=1)
        self.cmd_vel_pub = rospy.Publisher('/cmd_vel', Twist, queue_size=10)
        self.run_action_group_srv = rospy.ServiceProxy('/pug_control/run_action_group', SetActionName)
        time.sleep(0.1)
        self.gait_pub.publish(0.2, 0.15, 0.0, 0.05)
        self.pose_pub.publish(0, math.radians(-18), 0, -0.13, 0, 0, 0, 0.5)
        self.run()
```

(1) Subscribe to the camera topic `/csi_camera/image_rect_color` to receive images.

(2) Publish pose messages to the topic `/pug_control/pose` and gait messages to the topic `/pug_control/gait`.

(3) Publish velocity messages to the topic `/cmd_vel`.

* **Image Processing**

The image is processed using OpenCV library functions to extract the contours of the color to be recognized. Then, the center point coordinates of the detected target are calculated.

{lineno-start=90}
```python
            for r in self.roi:
                roi_h = self.roi_h_list[n]
                blobs = frame_gb[r[0]:r[1], r[2]:r[3]]
                frame_lab = cv2.cvtColor(blobs, cv2.COLOR_BGR2LAB)  # 将图像转换到LAB空间
                n += 1
                if self.line_color in self.lab_data:
                    frame_mask = cv2.inRange(frame_lab,
                                             (self.lab_data[self.line_color]['min'][0],
                                              self.lab_data[self.line_color]['min'][1],
                                              self.lab_data[self.line_color]['min'][2]),
                                             (self.lab_data[self.line_color]['max'][0],
                                              self.lab_data[self.line_color]['max'][1],
                                              self.lab_data[self.line_color]['max'][2]))  
                    opened = cv2.morphologyEx(frame_mask, cv2.MORPH_OPEN, np.ones((6, 6), np.uint8))  
                    closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, np.ones((6, 6), np.uint8))   
                    contours = cv2.findContours(closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_L1)[-2]   
                    max_contours = self.getAreaMaxContour(contours)[0]   
    
                    if max_contours is not None:   
                        rect = cv2.minAreaRect(max_contours)   
                        box = np.int0(cv2.boxPoints(rect))   
                        for i in range(4):
                            box[i, 1] = int(box[i, 1] + (n - 1) * roi_h + self.roi[0][0])
                        cv2.drawContours(image, [box], -1, (0, 0, 255, 255), 2)   
                        
                        pt1_x, pt1_y = box[0, 0], box[0, 1]
                        pt3_x, pt3_y = box[2, 0], box[2, 1]
    
                        center_x, center_y = (pt1_x + pt3_x) / 2, (pt1_y + pt3_y) / 2   
                        cv2.circle(image, (int(center_x), int(center_y)), 5, (0, 0, 255), -1)   
                        center_.append([center_x, center_y])
                        
                        centroid_x_sum += center_x * r[4]
                        weight_sum += r[4]
```

* **Movement Control**

Calculate the forward and turning speeds based on the center point coordinates, then publish velocity messages to control the movement of the machine.

{lineno-start=}
```python
            twist = Twist()
            twist.linear.x = 0.15
            if weight_sum is not 0:
                
                line_centerx = int(centroid_x_sum / weight_sum)
                deflection_angle = -math.atan((line_centerx - (img_w / 2.0)) / (img_h / 2.0))/2 
                # print(deflection_angle)
                # self.pid.SetPoint = 0.138
                self.pid.update(deflection_angle)
                twist.angular.z = common.set_range(-self.pid.output, -1.0, 1.0)
                self.cmd_vel_pub.publish(twist)
```

## 7.6 Autonomous Ball Shooting

### 7.6.1 Program Logic

Initialize the robot dog object, load the color threshold configuration file, define basic postures such as standing and bending, as well as gait parameters. Control the robot dog's movement in a thread, adjusting its advancement based on the recognized position of color blocks to move left and right.

Next, the main thread continuously loops to capture images, recognizing color blocks within the images, and passing them to the motion control thread. The camera captures the image, adjusts it to a suitable resolution, converts it to the LAB color space for color threshold segmentation, and identifies the contour with the largest area as the target. The center coordinates of its minimum circumscribed circle are obtained.

Finally, the robot dog moves towards the target position. Upon reaching the target, it transitions to the hitting state, adjusts its position as necessary, performs the hitting action, then transitions to the end state and exits the program. Key functions and classes utilized include Camera, color threshold processing, contour detection, and more. The primary objective is to control the robot dog's movement through color recognition to accomplish a simple ball delivery task. Colors, motion control logic, etc. can be customized as needed.

### 7.6.2 Operation Steps

:::{Note}
The input command should be case and space sensitive.
:::

(1) Start the robot dog, and access the robot system desktop using [NoMachine](Appendix.md).

(2) <img src="../_static/media/chapter_7/section_6/image3.png" /> Double-click to open the command-line terminal.

(3) Execute the command below to terminate the app auto-start service.

```bash
rosrun pug_example kick_ball_demo.py
```

(4) If you need to terminate this game, press **'Ctrl+C'** on the terminal. If the program cannot be stopped, please retry.

### 7.6.3 Program Outcome

:::{Note}
The default detection color is purple.
:::

Place the required balls on the field and position the ROSPug robot dog on the field, ensuring that the balls are within the camera's field of view. The distance between the balls and the robot should not exceed 1 meter.

### 7.6.4 Function Extension

* **Change Followed Color**

By default, the system recognizes the ball color as purple. If you need to change the recognized color, for example to red, please follow these steps:

(1) Execute the command `vim kick_ball_demo.py` and hit `Enter` key to edit the ball shooting program.

```bash
vim kick_ball_demo.py
```

(2) Locate the code as indicated in red frame below.

<img src="../_static/media/chapter_7/section_6/image7.png" class="common_img" />

(3) Press the **"i"** key to enter editing mode, then change **"purple"** to **"red"**. Once the modification is complete, press the **"Esc"** key. Next, enter **":wq"** and press `Enter` to save and exit.

<img src="../_static/media/chapter_7/section_6/image8.png" class="common_img" />

(4) To initiate the independent kicking game, enter the following command. You can restart the game to view the updated effects.

```bash
rosrun pug_example kick_ball_demo.py
```

### 7.6.5 Program Parameter Analysis

The source code of this program is saved in:[/home/hiwonder/pug/src/pug_example/scripts/almighty_functions/kick_ball_demo.py](../_static/source_code/detect_and_track.zip)

* **Subscribe to Publishing Topic**

{lineno-start=30}
```python
        self.image_sub = rospy.Subscriber("/csi_camera/image_rect_color", Image, self.image_callback, queue_size=1)
        self.pose_pub = rospy.Publisher('/pug_control/pose', Pose, queue_size=1)
        self.gait_pub = rospy.Publisher('/pug_control/gait', Gait, queue_size=1)
        self.velocity_pub = rospy.Publisher('/pug_control/velocity_move', Velocity, queue_size=1)
        self.run_action_group_srv = rospy.ServiceProxy('/pug_control/run_action_group', SetActionName)
        time.sleep(0.1)
```

(1) Subscribe to the camera topic `/csi_camera/image_rect_color` to receive images.

(2) Publish pose messages to the topic `/pug_control/pose` and gait messages to the topic `/pug_control/gait`.

* **Ball Recognition**

Run image processing in the main thread to obtain the ball's coordinate information and print it to the terminal. Then, use the ball's center coordinates to control the robot dog to track the ball, as shown in the figure below:

{lineno-start=111}
```python
     
    def run(self):
        while self.running:
            image = self.image_queue.get(block=True)

            if self.status == 'find_ball' or self.status == 'kick_ball':   
                GaussianBlur_img = cv2.GaussianBlur(image, (3, 3), 3)  
                frame_lab = cv2.cvtColor(GaussianBlur_img, cv2.COLOR_BGR2LAB)  
                frame_mask = cv2.inRange(frame_lab,
                                         (self.lab_data[self.target_color]['min'][0],
                                          self.lab_data[self.target_color]['min'][1],
                                          self.lab_data[self.target_color]['min'][2]),
                                         (self.lab_data[self.target_color]['max'][0],
                                          self.lab_data[self.target_color]['max'][1],
                                          self.lab_data[self.target_color]['max'][2]))   
                eroded = cv2.erode(frame_mask, cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3)))   
                dilated = cv2.dilate(eroded, cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3)))  
                contours = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[-2]   
                areaMax_contour = self.getAreaMaxContour(contours)[0]   

                if self.ball_position_queue.full():
                    self.ball_position_queue.get()
                if areaMax_contour is not None:
                    ball = cv2.minEnclosingCircle(areaMax_contour)   
                    ball_centerx = int(ball[0][0])
                    ball_centery = int(ball[0][1])
                    # print(ball_centerx, ball_centery, ball[1])
                    self.ball_position_queue.put([ball_centerx, ball_centery])
                    cv2.circle(image, (ball_centerx, ball_centery), int(ball[1]), (0, 255, 0), 2)
                    cv2.circle(image, (ball_centerx, ball_centery), 5, (0, 255, 0), -1)
                else:
                    self.ball_position_queue.put([-1, -1])
            cv2.imshow(self.name, image)
            cv2.waitKey(1)
        self.run_action_group_srv('stand')
        rospy.signal_shutdown('shutdown')
```

* **Ball Shooting Control**

{lineno-start=71}
```python
    def action_thread(self):
        while self.running:
            ball_position = self.ball_position_queue.get(block=True)
            if ball_position[0] != -1:
                ball_center_x, ball_center_y = ball_position[0], ball_position[1]
                # print(ball_center_x, ball_center_y)
                if self.status == 'find_ball':   
                    if ball_center_y < 250 and 200 < ball_center_x < 600: 
                        self.velocity_pub.publish(0.1, 0, 0, False)
                        time.sleep(0.5)
                    elif ball_center_y < 300 and 200 < ball_center_x < 600:  
                        self.velocity_pub.publish(0.05, 0, 0, False)
                        time.sleep(0.5)
                    elif ball_center_x > 600:   
                        self.velocity_pub.publish(0.01, 0, math.radians(-10), False)
                    elif ball_center_x < 540:   
                        self.velocity_pub.publish(0.01, 0, math.radians(10), False)
                    else:
                        self.status = 'kick_ball'
                elif self.status == 'kick_ball' :
                    if ball_center_y < 330:  
                        self.velocity_pub.publish(0.05, 0, 0, False)
                        time.sleep(0.5)
                    elif ball_center_x > 600:   
                        self.velocity_pub.publish(0.0, 0, math.radians(-10), False)
                    elif ball_center_x < 540:  
                        self.velocity_pub.publish(0.0, 0, math.radians(10), False)
                    else:
                        self.velocity_pub.publish(0.08, 0, 0, False)  
                        time.sleep(0.8)
                        self.velocity_pub.publish(0, 0, 0, True) 
                        time.sleep(0.5)
                        self.run_action_group_srv('stand')
                        self.run_action_group_srv('kick_ball_right')
                        self.status = 'End'
                elif self.status == 'End':   
                    time.sleep(0.01)
            else:
                time.sleep(0.01)
```

Run the robot's movement functions using multithreading. Once the ball's coordinates are obtained, control the robot's movement to approach and align with the ball for kicking. When the robot is correctly positioned, execute the kicking action sequence.